{
 "cells": [
  {
   "cell_type": "code",
   "id": "dd35395d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T07:54:32.114468Z",
     "start_time": "2025-10-15T07:54:32.109813Z"
    }
   },
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Any, Dict, Sequence\n",
    "from functools import partial\n",
    "\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "# [Set hyperparams here]\n",
    "@dataclass\n",
    "class HRMConfig:\n",
    "    vocab_size: int = 10  # Sudoku digits 0(unfilled) .. 9\n",
    "    seq_len: int = 82  # Sudoku has 9x9 = 81 cells + BOS\n",
    "\n",
    "    hidden_size: int = 256\n",
    "    intermediate_size: int = 256\n",
    "    head_dim: int = 64\n",
    "    is_causal: bool = False\n",
    "\n",
    "    num_layers: int = 4\n",
    "\n",
    "    H_cycles: int = 2\n",
    "    L_cycles: int = 2\n",
    "\n",
    "    norm_eps: float = 1e-6\n",
    "    rope_base: float = 10000.0\n",
    "    forward_dtype: str = \"bfloat16\"\n",
    "\n",
    "    batch_size: int = 256\n",
    "    cycle_per_data: int = 16\n",
    "    seed: int = 42"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T07:54:32.206273Z",
     "start_time": "2025-10-15T07:54:32.204314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_up(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ],
   "id": "207392badc9a48bf",
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "f8b0da36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T07:54:32.323572Z",
     "start_time": "2025-10-15T07:54:32.305604Z"
    }
   },
   "source": [
    "# [Model implementation]\n",
    "\n",
    "CosSin = Tuple[torch.Tensor, torch.Tensor]\n",
    "\n",
    "def trunc_normal_init_(x: torch.Tensor, std: float):\n",
    "    return nn.init.trunc_normal_(x, std=std).mul_(1.1368472343385565)  # Scale by a constant, so that actual std of the output is same as the specified std argument\n",
    "\n",
    "def rotate_half(x: torch.Tensor):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(x: torch.Tensor, cos_sin: CosSin):\n",
    "    # q, k: [..., seq_len, num_heads, head_dim]\n",
    "    # cos, sin: [seq_len, head_dim]\n",
    "    cos, sin = cos_sin\n",
    "    return ((x * cos.unsqueeze(-2)) + (rotate_half(x) * sin.unsqueeze(-2))).to(x.dtype)\n",
    "\n",
    "class CastedLinear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool, batch_output_dims: Sequence[int] = (), **kwargs):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            trunc_normal_init_(torch.empty((*batch_output_dims, out_features, in_features), **kwargs), std=1.0 / (in_features ** 0.5))\n",
    "        )\n",
    "        self.bias = None\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros((out_features, ), **kwargs))\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return F.linear(input, self.weight.view(-1, self.in_features).to(input.dtype), self.bias.to(input.dtype) if self.bias is not None else None)\n",
    "\n",
    "class CastedScaledEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, cast_to: torch.dtype):\n",
    "        super().__init__()\n",
    "        self.cast_to = cast_to\n",
    "\n",
    "        # Scale to the same std as most parameters\n",
    "        self.scale = embedding_dim ** 0.5\n",
    "        self.weight = nn.Parameter(\n",
    "            trunc_normal_init_(torch.empty((num_embeddings, embedding_dim)), std=1.0 / self.scale)\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return F.embedding(input, self.scale * self.weight.to(self.cast_to))\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings, base, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # RoPE\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32, device=device) / dim))\n",
    "        t = torch.arange(max_position_embeddings, dtype=torch.float32, device=device)\n",
    "        freqs = torch.outer(t, inv_freq)\n",
    "\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.cos_cached = nn.Buffer(emb.cos(), persistent=False)\n",
    "        self.sin_cached = nn.Buffer(emb.sin(), persistent=False)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.cos_cached, self.sin_cached\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, hidden_size: int, intermediate_size: int, **kwargs):\n",
    "        super().__init__()\n",
    "        self.gate_up_proj = CastedLinear(hidden_size, intermediate_size, bias=False, batch_output_dims=(2, ), **kwargs)\n",
    "        self.down_proj = CastedLinear(intermediate_size, hidden_size, bias=False, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate, up = self.gate_up_proj(x).chunk(2, dim=-1)\n",
    "        return self.down_proj(F.silu(gate) * up)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, head_dim, num_heads, is_causal, **kwargs):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.is_causal = is_causal\n",
    "\n",
    "        self.qkv_proj = CastedLinear(hidden_size, self.num_heads * self.head_dim, bias=False, batch_output_dims=(3, ), **kwargs)\n",
    "        self.o_proj = CastedLinear(head_dim * num_heads, hidden_size, bias=False, **kwargs)\n",
    "        with torch.no_grad():\n",
    "            self.o_proj.weight.zero_()\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, cos_sin: CosSin) -> torch.Tensor:\n",
    "        # hidden_states, qkv: [..., seq_len, hidden_size]\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "\n",
    "        # Split head (last dimension of projected qkv)\n",
    "        qkv = rearrange(qkv, \"... (h hd) -> ... h hd\", h=self.num_heads)\n",
    "        query, key, value = qkv.chunk(3, dim=-1)\n",
    "        # Rotary embedding\n",
    "        query = apply_rotary_pos_emb(query, cos_sin)\n",
    "        key = apply_rotary_pos_emb(key, cos_sin)\n",
    "        # PyTorch SDPA attention\n",
    "        # query, key, value: [... x seq_len x num_heads x head_dim]\n",
    "        attn_output = F.scaled_dot_product_attention(query.transpose(-2, -3), key.transpose(-2, -3), value.transpose(-2, -3), is_causal=self.is_causal).transpose(-2, -3)\n",
    "        # attn_output: [..., seq_len, num_heads, head_dim]\n",
    "        attn_output = rearrange(attn_output, \"... h hd -> ... (h hd)\")\n",
    "        return self.o_proj(attn_output)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: HRMConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.attn = Attention(\n",
    "            hidden_size=config.hidden_size,\n",
    "            head_dim=config.head_dim,\n",
    "            num_heads=config.hidden_size // config.head_dim,\n",
    "            is_causal=config.is_causal\n",
    "        )\n",
    "        self.mlp = SwiGLU(\n",
    "            hidden_size=config.hidden_size,\n",
    "            intermediate_size=config.intermediate_size\n",
    "        )\n",
    "        self.norm = lambda x: F.rms_norm(x, (x.shape[-1], ), eps=config.norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs) -> torch.Tensor:  # Post Norm\n",
    "        x = self.norm(x + self.attn(x, **kwargs))\n",
    "        return self.norm(x + self.mlp(x))\n",
    "\n",
    "class HRMRecurrentBlock(nn.Module):\n",
    "    def __init__(self, config: HRMConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([TransformerBlock(config) for _layer_idx in range(config.num_layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, n: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        h = x + n\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, **kwargs)\n",
    "        return h\n",
    "\n",
    "# HRMCarry is a tuple containing two latent states(z_H, z_L)\n",
    "HRMCarry = Tuple[torch.Tensor, torch.Tensor]\n",
    "\n",
    "class HRM(nn.Module):\n",
    "    def __init__(self, config: HRMConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.H_cycles = config.H_cycles\n",
    "        self.L_cycles = config.L_cycles\n",
    "\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.seq_len = config.seq_len\n",
    "        self.dtype = getattr(torch, config.forward_dtype)\n",
    "\n",
    "        # Backbone Layers\n",
    "        self.H_level = HRMRecurrentBlock(config)\n",
    "        self.L_level = HRMRecurrentBlock(config)\n",
    "        \n",
    "        # RoPE\n",
    "        self.rope = RotaryEmbedding(config.head_dim, config.seq_len, config.rope_base)\n",
    "        # I/O Layers\n",
    "        self.embed = CastedScaledEmbedding(config.vocab_size, config.hidden_size, cast_to=self.dtype)\n",
    "        self.lm_head = CastedLinear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "    def initial_carry(self, batch_size: int):\n",
    "        z_H = trunc_normal_init_(torch.empty(1, 1, self.hidden_size, dtype=self.dtype), std=1.0).expand(batch_size, self.seq_len, -1)\n",
    "        z_L = trunc_normal_init_(torch.empty(1, 1, self.hidden_size, dtype=self.dtype), std=1.0).expand(batch_size, self.seq_len, -1)\n",
    "        return (z_H, z_L)\n",
    "\n",
    "    def forward(self, carry: HRMCarry, input_ids: torch.Tensor) -> Tuple[HRMCarry, torch.Tensor]:\n",
    "        x = self.embed(input_ids)\n",
    "        seq_info = dict(cos_sin=self.rope())\n",
    "\n",
    "        # Forward iterations\n",
    "        with torch.no_grad():\n",
    "            z_H, z_L = carry  # Unpack tuple\n",
    "            for _i in range(self.H_cycles * self.L_cycles - 1):\n",
    "                z_L = self.L_level(z_L, z_H + x, **seq_info)\n",
    "                if (_i + 1) % self.L_cycles == 0:\n",
    "                    z_H = self.H_level(z_H, z_L, **seq_info)\n",
    "\n",
    "        assert not z_H.requires_grad and not z_L.requires_grad\n",
    "\n",
    "        # 1-step grad\n",
    "        z_L = self.L_level(z_L, z_H + x, **seq_info)\n",
    "        z_H = self.H_level(z_H, z_L, **seq_info)\n",
    "        return (z_H.detach(), z_L.detach()), self.lm_head(z_H)  # Return tuple and ensure no gradient moves across carry"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "74e1d14b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T07:54:32.433308Z",
     "start_time": "2025-10-15T07:54:32.431177Z"
    }
   },
   "source": [
    "# [Inference Step]\n",
    "@torch.inference_mode()\n",
    "def run_inference(model: nn.Module, carry: HRMCarry, x: torch.Tensor):\n",
    "    carry, y_hat = model(carry, x)\n",
    "    return carry, torch.argmax(y_hat, dim=-1)"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "3fdc75d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T07:54:32.565866Z",
     "start_time": "2025-10-15T07:54:32.559046Z"
    }
   },
   "source": [
    "# [Dataloader and training loop]\n",
    "def shuffle_sudoku(board: np.ndarray, solution: np.ndarray):\n",
    "    # Create a random digit mapping: a permutation of 1..9, with zero (blank) unchanged\n",
    "    digit_map = np.pad(np.random.permutation(np.arange(1, 10)), (1, 0))\n",
    "    \n",
    "    # Randomly decide whether to transpose.\n",
    "    transpose_flag = np.random.rand() < 0.5\n",
    "\n",
    "    # Generate a valid row permutation:\n",
    "    # - Shuffle the 3 bands (each band = 3 rows) and for each band, shuffle its 3 rows.\n",
    "    bands = np.random.permutation(3)\n",
    "    row_perm = np.concatenate([b * 3 + np.random.permutation(3) for b in bands])\n",
    "\n",
    "    # Similarly for columns (stacks).\n",
    "    stacks = np.random.permutation(3)\n",
    "    col_perm = np.concatenate([s * 3 + np.random.permutation(3) for s in stacks])\n",
    "\n",
    "    # Build an 81->81 mapping. For each new cell at (i, j)\n",
    "    # (row index = i // 9, col index = i % 9),\n",
    "    # its value comes from old row = row_perm[i//9] and old col = col_perm[i%9].\n",
    "    mapping = np.array([row_perm[i // 9] * 9 + col_perm[i % 9] for i in range(81)])\n",
    "\n",
    "    def apply_transformation(x: np.ndarray) -> np.ndarray:\n",
    "        # Apply transpose flag\n",
    "        if transpose_flag:\n",
    "            x = x.T\n",
    "        # Apply the position mapping.\n",
    "        new_board = x.flatten()[mapping].reshape(9, 9).copy()\n",
    "        # Apply digit mapping\n",
    "        return digit_map[new_board]\n",
    "\n",
    "    return apply_transformation(board), apply_transformation(solution)\n",
    "\n",
    "def collate_fn(batch: Dict[str, Any], augment: bool) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xs, ys = [], []\n",
    "    for item in batch:\n",
    "        board = np.frombuffer(item[\"question\"].replace('.', '0').encode(), dtype=np.uint8).reshape(9, 9) - ord('0')\n",
    "        solution = np.frombuffer(item[\"answer\"].encode(), dtype=np.uint8).reshape(9, 9) - ord('0')\n",
    "        if augment:\n",
    "            board, solution = shuffle_sudoku(board, solution)\n",
    "\n",
    "        # Convert and flatten\n",
    "        board = board.flatten().astype(np.int32)\n",
    "        solution = solution.flatten().astype(np.int32)\n",
    "        # Pad a BOS token\n",
    "        xs.append(np.pad(board, (1, 0)))\n",
    "        ys.append(np.pad(solution, (1, 0)))\n",
    "\n",
    "    return torch.from_numpy(np.stack(xs, axis=0)), torch.from_numpy(np.stack(ys, axis=0))\n",
    "\n",
    "def create_dataloader(split: str, batch_size: int, repeat: int = 1):\n",
    "    data_files = {\n",
    "        'train':'data/train.csv',\n",
    "        'test_hard':'data/test_hard.csv',\n",
    "        'test_sudoku_bench':'data/test_sudoku_bench.csv',\n",
    "    }\n",
    "    dataset = load_dataset('csv', data_files=data_files, split=split).repeat(repeat)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=partial(collate_fn, augment=split == \"train\"),\n",
    "        shuffle=True,\n",
    "        drop_last=len(dataset) >= batch_size,\n",
    "        num_workers=0,  # Set to 0 to avoid multiprocessing issues in Jupyter\n",
    "        prefetch_factor=None,\n",
    "        persistent_workers=False  # Must be False when num_workers=0\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T07:54:33.901421Z",
     "start_time": "2025-10-15T07:54:32.683989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hrm_config = HRMConfig()\n",
    "\n",
    "set_up(hrm_config.seed)\n",
    "\n",
    "device = torch.accelerator.current_accelerator(check_available=True)\n",
    "if device is None:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "with torch.device(device):\n",
    "    model = HRM(hrm_config)\n",
    "# load parameter\n",
    "model_path='HRM_Mini.pth'\n",
    "state_dict = torch.load(model_path, map_location=device)\n",
    "# as we compile the model. so here we remove the prefix\n",
    "model.load_state_dict({k.replace(\"_orig_mod.\", \"\"): v for k, v in state_dict.items()}, strict=True)"
   ],
   "id": "704cf037a61728c9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T08:00:34.245633Z",
     "start_time": "2025-10-15T08:00:03.121686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# evaluation\n",
    "model.eval()\n",
    "\n",
    "# Initialize\n",
    "eval_loaders = {split_name: create_dataloader(split_name, hrm_config.batch_size) for split_name in [\"test_hard\",\"test_sudoku_bench\"]}\n",
    "\n",
    "for eval_name, eval_loader in eval_loaders.items():\n",
    "    num_total = 0\n",
    "    num_correct = 0\n",
    "    for x, y in eval_loader:\n",
    "        with torch.device(device):\n",
    "            carry = model.initial_carry(x.shape[0])\n",
    "        for cycle in range(hrm_config.cycle_per_data):\n",
    "            carry, y_hat = model(carry, x.to(device))\n",
    "        y_hat = torch.argmax(y_hat, dim=-1)\n",
    "        num_total += y.shape[0]\n",
    "        num_correct += torch.all(y_hat == y.to(device), dim=-1).sum().item()\n",
    "    print (f\"[Eval Set {eval_name}]\", f\"Solved: {100 * num_correct / num_total:.2f}% ({num_correct}/{num_total})\")\n"
   ],
   "id": "55e54171b1a84b7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval Set test_hard] Solved: 44.65% (8916/19968)\n",
      "[Eval Set test_sudoku_bench] Solved: 90.00% (90/100)\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b21ea26f15dd3b27"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hrm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
