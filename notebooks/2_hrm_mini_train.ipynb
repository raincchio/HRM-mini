{
 "cells": [
  {
   "cell_type": "code",
   "id": "dd35395d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T08:23:05.452440Z",
     "start_time": "2025-10-15T08:23:05.445622Z"
    }
   },
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Any, Dict, Sequence\n",
    "from functools import partial\n",
    "\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "from muon import SingleDeviceMuon\n",
    "import random\n",
    "\n",
    "torch._dynamo.config.compiled_autograd = True\n",
    "\n",
    "# [Set hyperparams here]\n",
    "@dataclass\n",
    "class HRMConfig:\n",
    "    vocab_size: int = 10  # Sudoku digits 0(unfilled) .. 9\n",
    "    seq_len: int = 82  # Sudoku has 9x9 = 81 cells + BOS\n",
    "\n",
    "    hidden_size: int = 256\n",
    "    intermediate_size: int = 256\n",
    "    head_dim: int = 64\n",
    "    is_causal: bool = False\n",
    "\n",
    "    num_layers: int = 4\n",
    "\n",
    "    H_cycles: int = 2\n",
    "    L_cycles: int = 2\n",
    "\n",
    "    norm_eps: float = 1e-6\n",
    "    rope_base: float = 10000.0\n",
    "    forward_dtype: str = \"bfloat16\" # change to float32 if your hardware doesn't support\n",
    "\n",
    "    seed: int = 42\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    epochs: int = 5\n",
    "    train_dataset_repeat: int = 160\n",
    "    cycle_per_data: int = 16\n",
    "\n",
    "    batch_size: int = 256\n",
    "\n",
    "    lr: float = 0.001\n",
    "    weight_decay: float = 0.1\n",
    "\n",
    "    log_interval: int = 10\n"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T08:23:05.457383Z",
     "start_time": "2025-10-15T08:23:05.455276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_up(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ],
   "id": "13f693bec527d7d9",
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "id": "f8b0da36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T08:23:05.698456Z",
     "start_time": "2025-10-15T08:23:05.678415Z"
    }
   },
   "source": [
    "# [Model implementation]\n",
    "\n",
    "CosSin = Tuple[torch.Tensor, torch.Tensor]\n",
    "\n",
    "def trunc_normal_init_(x: torch.Tensor, std: float):\n",
    "    return nn.init.trunc_normal_(x, std=std).mul_(1.1368472343385565)  # Scale by a constant, so that actual std of the output is same as the specified std argument\n",
    "\n",
    "def rotate_half(x: torch.Tensor):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(x: torch.Tensor, cos_sin: CosSin):\n",
    "    # q, k: [..., seq_len, num_heads, head_dim]\n",
    "    # cos, sin: [seq_len, head_dim]\n",
    "    cos, sin = cos_sin\n",
    "    return ((x * cos.unsqueeze(-2)) + (rotate_half(x) * sin.unsqueeze(-2))).to(x.dtype)\n",
    "\n",
    "class CastedLinear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool, batch_output_dims: Sequence[int] = (), **kwargs):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            trunc_normal_init_(torch.empty((*batch_output_dims, out_features, in_features), **kwargs), std=1.0 / (in_features ** 0.5))\n",
    "        )\n",
    "        self.bias = None\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros((out_features, ), **kwargs))\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return F.linear(input, self.weight.view(-1, self.in_features).to(input.dtype), self.bias.to(input.dtype) if self.bias is not None else None)\n",
    "\n",
    "class CastedScaledEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, cast_to: torch.dtype):\n",
    "        super().__init__()\n",
    "        self.cast_to = cast_to\n",
    "\n",
    "        # Scale to the same std as most parameters\n",
    "        self.scale = embedding_dim ** 0.5\n",
    "        self.weight = nn.Parameter(\n",
    "            trunc_normal_init_(torch.empty((num_embeddings, embedding_dim)), std=1.0 / self.scale)\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return F.embedding(input, self.scale * self.weight.to(self.cast_to))\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings, base, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # RoPE\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32, device=device) / dim))\n",
    "        t = torch.arange(max_position_embeddings, dtype=torch.float32, device=device)\n",
    "        freqs = torch.outer(t, inv_freq)\n",
    "\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.cos_cached = nn.Buffer(emb.cos(), persistent=False)\n",
    "        self.sin_cached = nn.Buffer(emb.sin(), persistent=False)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.cos_cached, self.sin_cached\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, hidden_size: int, intermediate_size: int, **kwargs):\n",
    "        super().__init__()\n",
    "        self.gate_up_proj = CastedLinear(hidden_size, intermediate_size, bias=False, batch_output_dims=(2, ), **kwargs)\n",
    "        self.down_proj = CastedLinear(intermediate_size, hidden_size, bias=False, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate, up = self.gate_up_proj(x).chunk(2, dim=-1)\n",
    "        return self.down_proj(F.silu(gate) * up)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, head_dim, num_heads, is_causal, **kwargs):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.is_causal = is_causal\n",
    "\n",
    "        self.qkv_proj = CastedLinear(hidden_size, self.num_heads * self.head_dim, bias=False, batch_output_dims=(3, ), **kwargs)\n",
    "        self.o_proj = CastedLinear(head_dim * num_heads, hidden_size, bias=False, **kwargs)\n",
    "        with torch.no_grad():\n",
    "            self.o_proj.weight.zero_()\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, cos_sin: CosSin) -> torch.Tensor:\n",
    "        # hidden_states, qkv: [..., seq_len, hidden_size]\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "\n",
    "        # Split head (last dimension of projected qkv)\n",
    "        qkv = rearrange(qkv, \"... (h hd) -> ... h hd\", h=self.num_heads)\n",
    "        query, key, value = qkv.chunk(3, dim=-1)\n",
    "        # Rotary embedding\n",
    "        query = apply_rotary_pos_emb(query, cos_sin)\n",
    "        key = apply_rotary_pos_emb(key, cos_sin)\n",
    "        # PyTorch SDPA attention\n",
    "        # query, key, value: [... x seq_len x num_heads x head_dim]\n",
    "        attn_output = F.scaled_dot_product_attention(query.transpose(-2, -3), key.transpose(-2, -3), value.transpose(-2, -3), is_causal=self.is_causal).transpose(-2, -3)\n",
    "        # attn_output: [..., seq_len, num_heads, head_dim]\n",
    "        attn_output = rearrange(attn_output, \"... h hd -> ... (h hd)\")\n",
    "        return self.o_proj(attn_output)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: HRMConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.attn = Attention(\n",
    "            hidden_size=config.hidden_size,\n",
    "            head_dim=config.head_dim,\n",
    "            num_heads=config.hidden_size // config.head_dim,\n",
    "            is_causal=config.is_causal\n",
    "        )\n",
    "        self.mlp = SwiGLU(\n",
    "            hidden_size=config.hidden_size,\n",
    "            intermediate_size=config.intermediate_size\n",
    "        )\n",
    "        self.norm = lambda x: F.rms_norm(x, (x.shape[-1], ), eps=config.norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs) -> torch.Tensor:  # Post Norm\n",
    "        x = self.norm(x + self.attn(x, **kwargs))\n",
    "        return self.norm(x + self.mlp(x))\n",
    "\n",
    "class HRMRecurrentBlock(nn.Module):\n",
    "    def __init__(self, config: HRMConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([TransformerBlock(config) for _layer_idx in range(config.num_layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, n: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        h = x + n\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, **kwargs)\n",
    "        return h\n",
    "\n",
    "# HRMCarry is a tuple containing two latent states(z_H, z_L)\n",
    "HRMCarry = Tuple[torch.Tensor, torch.Tensor]\n",
    "\n",
    "class HRM(nn.Module):\n",
    "    def __init__(self, config: HRMConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.H_cycles = config.H_cycles\n",
    "        self.L_cycles = config.L_cycles\n",
    "\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.seq_len = config.seq_len\n",
    "        self.dtype = getattr(torch, config.forward_dtype)\n",
    "\n",
    "        # Backbone Layers\n",
    "        self.H_level = HRMRecurrentBlock(config)\n",
    "        self.L_level = HRMRecurrentBlock(config)\n",
    "        \n",
    "        # RoPE\n",
    "        self.rope = RotaryEmbedding(config.head_dim, config.seq_len, config.rope_base)\n",
    "        # I/O Layers\n",
    "        self.embed = CastedScaledEmbedding(config.vocab_size, config.hidden_size, cast_to=self.dtype)\n",
    "        self.lm_head = CastedLinear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "    def initial_carry(self, batch_size: int):\n",
    "        z_H = trunc_normal_init_(torch.empty(1, 1, self.hidden_size, dtype=self.dtype), std=1.0).expand(batch_size, self.seq_len, -1)\n",
    "        z_L = trunc_normal_init_(torch.empty(1, 1, self.hidden_size, dtype=self.dtype), std=1.0).expand(batch_size, self.seq_len, -1)\n",
    "        return (z_H, z_L)\n",
    "\n",
    "    def forward(self, carry: HRMCarry, input_ids: torch.Tensor) -> Tuple[HRMCarry, torch.Tensor]:\n",
    "        x = self.embed(input_ids)\n",
    "        seq_info = dict(cos_sin=self.rope())\n",
    "\n",
    "        # Forward iterations\n",
    "        with torch.no_grad():\n",
    "            z_H, z_L = carry  # Unpack tuple\n",
    "            for _i in range(self.H_cycles * self.L_cycles - 1):\n",
    "                z_L = self.L_level(z_L, z_H + x, **seq_info)\n",
    "                if (_i + 1) % self.L_cycles == 0:\n",
    "                    z_H = self.H_level(z_H, z_L, **seq_info)\n",
    "\n",
    "        assert not z_H.requires_grad and not z_L.requires_grad\n",
    "\n",
    "        # 1-step grad\n",
    "        z_L = self.L_level(z_L, z_H + x, **seq_info)\n",
    "        z_H = self.H_level(z_H, z_L, **seq_info)\n",
    "        return (z_H.detach(), z_L.detach()), self.lm_head(z_H)  # Return tuple and ensure no gradient moves across carry"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "id": "74e1d14b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T08:23:05.819024Z",
     "start_time": "2025-10-15T08:23:05.813426Z"
    }
   },
   "source": [
    "# [Training and Inference Step]\n",
    "@torch.compile(dynamic=False)\n",
    "def train_step(model: nn.Module, carry: HRMCarry, opt: torch.optim.Optimizer, x: torch.Tensor, y: torch.Tensor):\n",
    "    carry, y_hat = model(carry, x)\n",
    "    # loss (f32 for CrossEntropy)\n",
    "    loss = F.cross_entropy(y_hat.view(-1, y_hat.shape[-1]).to(torch.float32), y.view(-1), reduction=\"mean\")\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # metrics\n",
    "    with torch.no_grad():\n",
    "        preds = torch.argmax(y_hat, dim=-1)\n",
    "        metrics = {\n",
    "            \"loss\": loss.detach(),\n",
    "            \"per_position_accuracy\": torch.mean(preds == y, dtype=torch.float32),\n",
    "            \"exact_match\": torch.mean(torch.all(preds == y, dim=-1), dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "    return carry, metrics\n",
    "\n",
    "@torch.inference_mode()\n",
    "def run_inference(model: nn.Module, carry: HRMCarry, x: torch.Tensor):\n",
    "    carry, y_hat = model(carry, x)\n",
    "    return carry, torch.argmax(y_hat, dim=-1)"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "id": "3fdc75d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T08:23:05.972874Z",
     "start_time": "2025-10-15T08:23:05.965109Z"
    }
   },
   "source": [
    "# [Dataloader and training loop]\n",
    "def shuffle_sudoku(board: np.ndarray, solution: np.ndarray):\n",
    "    # Create a random digit mapping: a permutation of 1..9, with zero (blank) unchanged\n",
    "    digit_map = np.pad(np.random.permutation(np.arange(1, 10)), (1, 0))\n",
    "    \n",
    "    # Randomly decide whether to transpose.\n",
    "    transpose_flag = np.random.rand() < 0.5\n",
    "\n",
    "    # Generate a valid row permutation:\n",
    "    # - Shuffle the 3 bands (each band = 3 rows) and for each band, shuffle its 3 rows.\n",
    "    bands = np.random.permutation(3)\n",
    "    row_perm = np.concatenate([b * 3 + np.random.permutation(3) for b in bands])\n",
    "\n",
    "    # Similarly for columns (stacks).\n",
    "    stacks = np.random.permutation(3)\n",
    "    col_perm = np.concatenate([s * 3 + np.random.permutation(3) for s in stacks])\n",
    "\n",
    "    # Build an 81->81 mapping. For each new cell at (i, j)\n",
    "    # (row index = i // 9, col index = i % 9),\n",
    "    # its value comes from old row = row_perm[i//9] and old col = col_perm[i%9].\n",
    "    mapping = np.array([row_perm[i // 9] * 9 + col_perm[i % 9] for i in range(81)])\n",
    "\n",
    "    def apply_transformation(x: np.ndarray) -> np.ndarray:\n",
    "        # Apply transpose flag\n",
    "        if transpose_flag:\n",
    "            x = x.T\n",
    "        # Apply the position mapping.\n",
    "        new_board = x.flatten()[mapping].reshape(9, 9).copy()\n",
    "        # Apply digit mapping\n",
    "        return digit_map[new_board]\n",
    "\n",
    "    return apply_transformation(board), apply_transformation(solution)\n",
    "\n",
    "def collate_fn(batch: Dict[str, Any], augment: bool) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xs, ys = [], []\n",
    "    for item in batch:\n",
    "        board = np.frombuffer(item[\"question\"].replace('.', '0').encode(), dtype=np.uint8).reshape(9, 9) - ord('0')\n",
    "        solution = np.frombuffer(item[\"answer\"].encode(), dtype=np.uint8).reshape(9, 9) - ord('0')\n",
    "        if augment:\n",
    "            board, solution = shuffle_sudoku(board, solution)\n",
    "\n",
    "        # Convert and flatten\n",
    "        board = board.flatten().astype(np.int32)\n",
    "        solution = solution.flatten().astype(np.int32)\n",
    "        # Pad a BOS token\n",
    "        xs.append(np.pad(board, (1, 0)))\n",
    "        ys.append(np.pad(solution, (1, 0)))\n",
    "\n",
    "    return torch.from_numpy(np.stack(xs, axis=0)), torch.from_numpy(np.stack(ys, axis=0))\n",
    "\n",
    "def create_dataloader(split: str, batch_size: int, repeat: int = 1):\n",
    "\n",
    "    data_files = {\n",
    "        'train':'data/train.csv',\n",
    "        'test_hard':'data/test_hard.csv',\n",
    "        'test_sudoku_bench':'data/test_sudoku_bench.csv',\n",
    "    }\n",
    "    dataset = load_dataset('csv', data_files=data_files, split=split).repeat(repeat)\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=partial(collate_fn, augment=split == \"train\"),\n",
    "        shuffle=True,\n",
    "        drop_last=len(dataset) >= batch_size,\n",
    "        num_workers=0,  # Set to 0 to avoid multiprocessing issues in Jupyter\n",
    "        prefetch_factor=None,\n",
    "        persistent_workers=False  # Must be False when num_workers=0\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T08:23:55.280441Z",
     "start_time": "2025-10-15T08:23:06.120254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# traing model\n",
    "model_config, train_config = HRMConfig(), TrainConfig()\n",
    "set_up(model_config.seed)\n",
    "device = torch.accelerator.current_accelerator(check_available=True)\n",
    "if device is None:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print (f\"Training on {device.type}\")\n",
    "\n",
    "# Initialize\n",
    "train_loader = create_dataloader(\"train\", train_config.batch_size, repeat=train_config.train_dataset_repeat)\n",
    "total_steps = int(train_config.cycle_per_data * len(train_loader) * train_config.epochs)\n",
    "\n",
    "with torch.device(device):\n",
    "    model = HRM(model_config)\n",
    "    carry = model.initial_carry(train_config.batch_size)\n",
    "    # Inner compile wrap\n",
    "    model = torch.compile(model, dynamic=False, fullgraph=True)\n",
    "\n",
    "opt = SingleDeviceMuon(\n",
    "    model.parameters(),\n",
    "    lr=train_config.lr,\n",
    "    weight_decay=train_config.weight_decay,\n",
    ")\n",
    "\n",
    "# Train & Eval loop\n",
    "step = 0\n",
    "last_log_step = 0\n",
    "\n",
    "for epoch in range(train_config.epochs):\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.long().to(device)\n",
    "\n",
    "        with torch.device(device):\n",
    "            carry = model.initial_carry(x.shape[0])\n",
    "\n",
    "        for cycle in range(train_config.cycle_per_data):\n",
    "            carry, metrics = train_step(model, carry, opt, x, y)\n",
    "            step += 1\n",
    "\n",
    "        if step - last_log_step >= train_config.log_interval:\n",
    "            last_log_step = step\n",
    "            print (f\"Ep {epoch} Step {step}/{total_steps}:\", ', '.join(f'{k}={v.item() if isinstance(v, torch.Tensor) else v:.3f}' for k, v in metrics.items()))\n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), \"../model/HRM_Mini.pth\")"
   ],
   "id": "77cf8fe865362ec8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Ep 0 Step 16/50000: loss=2.675, per_position_accuracy=0.125, exact_match=0.000\n",
      "Ep 0 Step 32/50000: loss=2.439, per_position_accuracy=0.177, exact_match=0.000\n",
      "Ep 0 Step 48/50000: loss=2.189, per_position_accuracy=0.257, exact_match=0.000\n",
      "Ep 0 Step 64/50000: loss=1.937, per_position_accuracy=0.338, exact_match=0.000\n",
      "Ep 0 Step 80/50000: loss=1.675, per_position_accuracy=0.374, exact_match=0.000\n",
      "Ep 0 Step 96/50000: loss=1.575, per_position_accuracy=0.388, exact_match=0.000\n",
      "Ep 0 Step 112/50000: loss=1.549, per_position_accuracy=0.400, exact_match=0.000\n",
      "Ep 0 Step 128/50000: loss=1.524, per_position_accuracy=0.415, exact_match=0.000\n",
      "Ep 0 Step 144/50000: loss=1.505, per_position_accuracy=0.420, exact_match=0.000\n",
      "Ep 0 Step 160/50000: loss=1.472, per_position_accuracy=0.425, exact_match=0.000\n",
      "Ep 0 Step 176/50000: loss=1.438, per_position_accuracy=0.433, exact_match=0.000\n",
      "Ep 0 Step 192/50000: loss=1.412, per_position_accuracy=0.435, exact_match=0.000\n",
      "Ep 0 Step 208/50000: loss=1.391, per_position_accuracy=0.436, exact_match=0.000\n",
      "Ep 0 Step 224/50000: loss=1.384, per_position_accuracy=0.443, exact_match=0.000\n",
      "Ep 0 Step 240/50000: loss=1.368, per_position_accuracy=0.447, exact_match=0.000\n",
      "Ep 0 Step 256/50000: loss=1.372, per_position_accuracy=0.443, exact_match=0.000\n",
      "Ep 0 Step 272/50000: loss=1.359, per_position_accuracy=0.450, exact_match=0.000\n",
      "Ep 0 Step 288/50000: loss=1.362, per_position_accuracy=0.452, exact_match=0.000\n",
      "Ep 0 Step 304/50000: loss=1.352, per_position_accuracy=0.453, exact_match=0.000\n",
      "Ep 0 Step 320/50000: loss=1.357, per_position_accuracy=0.458, exact_match=0.000\n",
      "Ep 0 Step 336/50000: loss=1.342, per_position_accuracy=0.460, exact_match=0.000\n",
      "Ep 0 Step 352/50000: loss=1.354, per_position_accuracy=0.460, exact_match=0.000\n",
      "Ep 0 Step 368/50000: loss=1.329, per_position_accuracy=0.463, exact_match=0.000\n",
      "Ep 0 Step 384/50000: loss=1.324, per_position_accuracy=0.469, exact_match=0.000\n",
      "Ep 0 Step 400/50000: loss=1.326, per_position_accuracy=0.466, exact_match=0.000\n",
      "Ep 0 Step 416/50000: loss=1.320, per_position_accuracy=0.462, exact_match=0.000\n",
      "Ep 0 Step 432/50000: loss=1.309, per_position_accuracy=0.472, exact_match=0.000\n",
      "Ep 0 Step 448/50000: loss=1.317, per_position_accuracy=0.464, exact_match=0.000\n",
      "Ep 0 Step 464/50000: loss=1.303, per_position_accuracy=0.472, exact_match=0.000\n",
      "Ep 0 Step 480/50000: loss=1.305, per_position_accuracy=0.472, exact_match=0.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[49]\u001B[39m\u001B[32m, line 40\u001B[39m\n\u001B[32m     37\u001B[39m     carry = model.initial_carry(x.shape[\u001B[32m0\u001B[39m])\n\u001B[32m     39\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m cycle \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(train_config.cycle_per_data):\n\u001B[32m---> \u001B[39m\u001B[32m40\u001B[39m     carry, metrics = \u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcarry\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     41\u001B[39m     step += \u001B[32m1\u001B[39m\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m step - last_log_step >= train_config.log_interval:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/vepfs-dev/xing/miniconda3/envs/test/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:736\u001B[39m, in \u001B[36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    733\u001B[39m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001B[32m    735\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m736\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    737\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m Unsupported \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    738\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m config.verbose:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[47]\u001B[39m\u001B[32m, line 2\u001B[39m, in \u001B[36mtrain_step\u001B[39m\u001B[34m(model, carry, opt, x, y)\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# [Training and Inference Step]\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;129m@torch\u001B[39m.compile(dynamic=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtrain_step\u001B[39m(model: nn.Module, carry: HRMCarry, opt: torch.optim.Optimizer, x: torch.Tensor, y: torch.Tensor):\n\u001B[32m      4\u001B[39m     carry, y_hat = model(carry, x)\n\u001B[32m      5\u001B[39m     \u001B[38;5;66;03m# loss (f32 for CrossEntropy)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/vepfs-dev/xing/miniconda3/envs/test/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929\u001B[39m, in \u001B[36mDisableContext.__call__.<locals>._fn\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    927\u001B[39m _maybe_set_eval_frame(_callback_from_stance(\u001B[38;5;28mself\u001B[39m.callback))\n\u001B[32m    928\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m929\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    930\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    931\u001B[39m     set_eval_frame(\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/vepfs-dev/xing/miniconda3/envs/test/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py:1241\u001B[39m, in \u001B[36maot_module_simplified.<locals>.forward\u001B[39m\u001B[34m(*runtime_args)\u001B[39m\n\u001B[32m   1239\u001B[39m full_args.extend(params_flat)\n\u001B[32m   1240\u001B[39m full_args.extend(runtime_args)\n\u001B[32m-> \u001B[39m\u001B[32m1241\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcompiled_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfull_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/vepfs-dev/xing/miniconda3/envs/test/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:370\u001B[39m, in \u001B[36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001B[39m\u001B[34m(args)\u001B[39m\n\u001B[32m    365\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m (\n\u001B[32m    366\u001B[39m         torch.autograd._force_original_view_tracking(\u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[32m    367\u001B[39m         torch.enable_grad(),\n\u001B[32m    368\u001B[39m     ):\n\u001B[32m    369\u001B[39m         record_runtime_wrapper_prologue_exit(cm)\n\u001B[32m--> \u001B[39m\u001B[32m370\u001B[39m         all_outs = \u001B[43mcall_func_at_runtime_with_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    371\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcompiled_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdisable_amp\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdisable_amp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msteal_args\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[32m    372\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    373\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    374\u001B[39m     \u001B[38;5;66;03m# When we have an inference graph, we run with grad disabled.\u001B[39;00m\n\u001B[32m    375\u001B[39m     \u001B[38;5;66;03m# It's possible to get an inference graph with inputs that require grad,\u001B[39;00m\n\u001B[32m    376\u001B[39m     \u001B[38;5;66;03m# in which case we want to make sure autograd is disabled\u001B[39;00m\n\u001B[32m    377\u001B[39m     \u001B[38;5;66;03m# (since e.g., inductor will generate aten.addmm.out calls which autograd will complain on)\u001B[39;00m\n\u001B[32m    378\u001B[39m     \u001B[38;5;66;03m# NOTE: We use _set_grad_enabled directly to reduce runtime overhead\u001B[39;00m\n\u001B[32m    379\u001B[39m     grad_enabled = torch.is_grad_enabled()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/vepfs-dev/xing/miniconda3/envs/test/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001B[39m, in \u001B[36mcall_func_at_runtime_with_args\u001B[39m\u001B[34m(f, args, steal_args, disable_amp)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[32m    125\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(f, \u001B[33m\"\u001B[39m\u001B[33m_boxed_call\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[32m--> \u001B[39m\u001B[32m126\u001B[39m         out = normalize_as_list(\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    127\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    128\u001B[39m         \u001B[38;5;66;03m# TODO: Please remove soon\u001B[39;00m\n\u001B[32m    129\u001B[39m         \u001B[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001B[39;00m\n\u001B[32m    130\u001B[39m         warnings.warn(\n\u001B[32m    131\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mYour compiler for AOTAutograd is returning a function that doesn\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt take boxed arguments. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    132\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    133\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    134\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/vepfs-dev/xing/miniconda3/envs/test/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:100\u001B[39m, in \u001B[36mmake_boxed_func.<locals>.g\u001B[39m\u001B[34m(args)\u001B[39m\n\u001B[32m     99\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mg\u001B[39m(args):\n\u001B[32m--> \u001B[39m\u001B[32m100\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/vepfs-dev/xing/miniconda3/envs/test/lib/python3.12/site-packages/torch/autograd/function.py:576\u001B[39m, in \u001B[36mFunction.apply\u001B[39m\u001B[34m(cls, *args, **kwargs)\u001B[39m\n\u001B[32m    573\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch._C._are_functorch_transforms_active():\n\u001B[32m    574\u001B[39m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n\u001B[32m    575\u001B[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001B[32m--> \u001B[39m\u001B[32m576\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m    578\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_setup_ctx_defined:\n\u001B[32m    579\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    580\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    581\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    582\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mstaticmethod. For more details, please see \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    583\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    584\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/vepfs-dev/xing/miniconda3/envs/test/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2074\u001B[39m, in \u001B[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.forward\u001B[39m\u001B[34m(ctx, *deduped_flat_tensor_args)\u001B[39m\n\u001B[32m   2065\u001B[39m     args = (*args, *fwd_rng_states)\n\u001B[32m   2067\u001B[39m \u001B[38;5;66;03m# There is a pretty complicated calling convention around what the compiled fw returns.\u001B[39;00m\n\u001B[32m   2068\u001B[39m \u001B[38;5;66;03m# The full list of outputs and their relative order is:\u001B[39;00m\n\u001B[32m   2069\u001B[39m \u001B[38;5;66;03m# (*tokens, *mutated_inputs, *fw_outs, *fw_intermediate_bases, *saved_tensors, *saved_symints)\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   2072\u001B[39m \u001B[38;5;66;03m# - Note that donated buffer logic requires (*saved_tensors, *saved_symints) showing up last\u001B[39;00m\n\u001B[32m   2073\u001B[39m \u001B[38;5;66;03m#   in the fw output order.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2074\u001B[39m fw_outs = \u001B[43mcall_func_at_runtime_with_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2075\u001B[39m \u001B[43m    \u001B[49m\u001B[43mCompiledFunction\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcompiled_fw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2076\u001B[39m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2077\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdisable_amp\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdisable_amp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2078\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2080\u001B[39m num_outputs = CompiledFunction.metadata.num_outputs\n\u001B[32m   2081\u001B[39m num_outputs_aliased = CompiledFunction.metadata.num_outputs_aliased\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/vepfs-dev/xing/miniconda3/envs/test/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001B[39m, in \u001B[36mcall_func_at_runtime_with_args\u001B[39m\u001B[34m(f, args, steal_args, disable_amp)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[32m    125\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(f, \u001B[33m\"\u001B[39m\u001B[33m_boxed_call\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[32m--> \u001B[39m\u001B[32m126\u001B[39m         out = normalize_as_list(\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    127\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    128\u001B[39m         \u001B[38;5;66;03m# TODO: Please remove soon\u001B[39;00m\n\u001B[32m    129\u001B[39m         \u001B[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001B[39;00m\n\u001B[32m    130\u001B[39m         warnings.warn(\n\u001B[32m    131\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mYour compiler for AOTAutograd is returning a function that doesn\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt take boxed arguments. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    132\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    133\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    134\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/vepfs-dev/xing/miniconda3/envs/test/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:556\u001B[39m, in \u001B[36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001B[39m\u001B[34m(runtime_args)\u001B[39m\n\u001B[32m    549\u001B[39m     out = \u001B[38;5;28mself\u001B[39m._functionalized_rng_runtime_epilogue(\n\u001B[32m    550\u001B[39m         runtime_metadata,\n\u001B[32m    551\u001B[39m         out,\n\u001B[32m    552\u001B[39m         \u001B[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001B[39;00m\n\u001B[32m    553\u001B[39m         runtime_metadata.num_forward_returns,\n\u001B[32m    554\u001B[39m     )\n\u001B[32m    555\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[32m--> \u001B[39m\u001B[32m556\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcompiled_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mruntime_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/vepfs-dev/xing/miniconda3/envs/test/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:750\u001B[39m, in \u001B[36mEffectTokensWrapper.post_compile.<locals>.inner_fn\u001B[39m\u001B[34m(args)\u001B[39m\n\u001B[32m    747\u001B[39m     args = [*([\u001B[38;5;28;01mNone\u001B[39;00m] * num_tokens), *args]\n\u001B[32m    748\u001B[39m     old_args.clear()\n\u001B[32m--> \u001B[39m\u001B[32m750\u001B[39m outs = \u001B[43mcompiled_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    752\u001B[39m \u001B[38;5;66;03m# Inductor cache DummyModule can return None\u001B[39;00m\n\u001B[32m    753\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m outs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/vepfs-dev/xing/miniconda3/envs/test/lib/python3.12/site-packages/torch/_inductor/output_code.py:584\u001B[39m, in \u001B[36mCompiledFxGraph.__call__\u001B[39m\u001B[34m(self, inputs)\u001B[39m\n\u001B[32m    582\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.current_callable \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    583\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m584\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcurrent_callable\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    585\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    586\u001B[39m     get_runtime_metrics_context().finish()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/vepfs-dev/xing/miniconda3/envs/test/lib/python3.12/site-packages/torch/_inductor/utils.py:2716\u001B[39m, in \u001B[36malign_inputs_from_check_idxs.<locals>.run\u001B[39m\u001B[34m(new_inputs)\u001B[39m\n\u001B[32m   2712\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrun\u001B[39m(new_inputs: \u001B[38;5;28mlist\u001B[39m[InputType]) -> Any:\n\u001B[32m   2713\u001B[39m     old_tensors, new_tensors = copy_misaligned_inputs(\n\u001B[32m   2714\u001B[39m         new_inputs, inputs_to_check, mutated_input_idxs\n\u001B[32m   2715\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m2716\u001B[39m     out = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2718\u001B[39m     \u001B[38;5;66;03m# If a mutated tensor was cloned to be aligned, we need to reflect back the mutation to the\u001B[39;00m\n\u001B[32m   2719\u001B[39m     \u001B[38;5;66;03m# original tensor.\u001B[39;00m\n\u001B[32m   2720\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(old_tensors):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/tmp/torchinductor_root/ai/caiacgcownsip6jyfu7jyigzxhej6mhwnhseum4aiawvpxo6zwsb.py:1755\u001B[39m, in \u001B[36mcall\u001B[39m\u001B[34m(args)\u001B[39m\n\u001B[32m   1753\u001B[39m buf231 = buf210; \u001B[38;5;28;01mdel\u001B[39;00m buf210  \u001B[38;5;66;03m# reuse\u001B[39;00m\n\u001B[32m   1754\u001B[39m \u001B[38;5;66;03m# Topologically Sorted Source Nodes: [linear_46], Original ATen: [aten.mm]\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1755\u001B[39m \u001B[43mextern_kernels\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreinterpret_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuf230\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m20992\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m256\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m256\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuf229\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbuf231\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1756\u001B[39m buf232 = empty_strided_cuda((\u001B[32m256\u001B[39m, \u001B[32m256\u001B[39m), (\u001B[32m1\u001B[39m, \u001B[32m256\u001B[39m), torch.bfloat16)\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# Topologically Sorted Source Nodes: [to_72, linear_47], Original ATen: [aten._to_copy, aten.t]\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 49
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hrm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
