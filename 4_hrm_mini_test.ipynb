{
 "cells": [
  {
   "cell_type": "code",
   "id": "dd35395d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T08:13:35.320643Z",
     "start_time": "2025-10-15T08:13:35.315622Z"
    }
   },
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Any, Dict, Sequence\n",
    "from functools import partial\n",
    "\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "# [Set hyperparams here]\n",
    "@dataclass\n",
    "class HRMConfig:\n",
    "    vocab_size: int = 10  # Sudoku digits 0(unfilled) .. 9\n",
    "    seq_len: int = 82  # Sudoku has 9x9 = 81 cells + BOS\n",
    "\n",
    "    hidden_size: int = 256\n",
    "    intermediate_size: int = 256\n",
    "    head_dim: int = 64\n",
    "    is_causal: bool = False\n",
    "\n",
    "    num_layers: int = 4\n",
    "\n",
    "    H_cycles: int = 2\n",
    "    L_cycles: int = 2\n",
    "\n",
    "    norm_eps: float = 1e-6\n",
    "    rope_base: float = 10000.0\n",
    "    forward_dtype: str = \"bfloat16\"\n",
    "\n",
    "    batch_size: int = 256\n",
    "    cycle_per_data: int = 16\n",
    "    seed: int = 42"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T08:13:35.366418Z",
     "start_time": "2025-10-15T08:13:35.364356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_up(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ],
   "id": "207392badc9a48bf",
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "f8b0da36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T08:13:35.597582Z",
     "start_time": "2025-10-15T08:13:35.580056Z"
    }
   },
   "source": [
    "# [Model implementation]\n",
    "\n",
    "CosSin = Tuple[torch.Tensor, torch.Tensor]\n",
    "\n",
    "def trunc_normal_init_(x: torch.Tensor, std: float):\n",
    "    return nn.init.trunc_normal_(x, std=std).mul_(1.1368472343385565)  # Scale by a constant, so that actual std of the output is same as the specified std argument\n",
    "\n",
    "def rotate_half(x: torch.Tensor):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(x: torch.Tensor, cos_sin: CosSin):\n",
    "    # q, k: [..., seq_len, num_heads, head_dim]\n",
    "    # cos, sin: [seq_len, head_dim]\n",
    "    cos, sin = cos_sin\n",
    "    return ((x * cos.unsqueeze(-2)) + (rotate_half(x) * sin.unsqueeze(-2))).to(x.dtype)\n",
    "\n",
    "class CastedLinear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool, batch_output_dims: Sequence[int] = (), **kwargs):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            trunc_normal_init_(torch.empty((*batch_output_dims, out_features, in_features), **kwargs), std=1.0 / (in_features ** 0.5))\n",
    "        )\n",
    "        self.bias = None\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros((out_features, ), **kwargs))\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return F.linear(input, self.weight.view(-1, self.in_features).to(input.dtype), self.bias.to(input.dtype) if self.bias is not None else None)\n",
    "\n",
    "class CastedScaledEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, cast_to: torch.dtype):\n",
    "        super().__init__()\n",
    "        self.cast_to = cast_to\n",
    "\n",
    "        # Scale to the same std as most parameters\n",
    "        self.scale = embedding_dim ** 0.5\n",
    "        self.weight = nn.Parameter(\n",
    "            trunc_normal_init_(torch.empty((num_embeddings, embedding_dim)), std=1.0 / self.scale)\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return F.embedding(input, self.scale * self.weight.to(self.cast_to))\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings, base, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # RoPE\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32, device=device) / dim))\n",
    "        t = torch.arange(max_position_embeddings, dtype=torch.float32, device=device)\n",
    "        freqs = torch.outer(t, inv_freq)\n",
    "\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.cos_cached = nn.Buffer(emb.cos(), persistent=False)\n",
    "        self.sin_cached = nn.Buffer(emb.sin(), persistent=False)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.cos_cached, self.sin_cached\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, hidden_size: int, intermediate_size: int, **kwargs):\n",
    "        super().__init__()\n",
    "        self.gate_up_proj = CastedLinear(hidden_size, intermediate_size, bias=False, batch_output_dims=(2, ), **kwargs)\n",
    "        self.down_proj = CastedLinear(intermediate_size, hidden_size, bias=False, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate, up = self.gate_up_proj(x).chunk(2, dim=-1)\n",
    "        return self.down_proj(F.silu(gate) * up)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, head_dim, num_heads, is_causal, **kwargs):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.is_causal = is_causal\n",
    "\n",
    "        self.qkv_proj = CastedLinear(hidden_size, self.num_heads * self.head_dim, bias=False, batch_output_dims=(3, ), **kwargs)\n",
    "        self.o_proj = CastedLinear(head_dim * num_heads, hidden_size, bias=False, **kwargs)\n",
    "        with torch.no_grad():\n",
    "            self.o_proj.weight.zero_()\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, cos_sin: CosSin) -> torch.Tensor:\n",
    "        # hidden_states, qkv: [..., seq_len, hidden_size]\n",
    "        qkv = self.qkv_proj(hidden_states)\n",
    "\n",
    "        # Split head (last dimension of projected qkv)\n",
    "        qkv = rearrange(qkv, \"... (h hd) -> ... h hd\", h=self.num_heads)\n",
    "        query, key, value = qkv.chunk(3, dim=-1)\n",
    "        # Rotary embedding\n",
    "        query = apply_rotary_pos_emb(query, cos_sin)\n",
    "        key = apply_rotary_pos_emb(key, cos_sin)\n",
    "        # PyTorch SDPA attention\n",
    "        # query, key, value: [... x seq_len x num_heads x head_dim]\n",
    "        attn_output = F.scaled_dot_product_attention(query.transpose(-2, -3), key.transpose(-2, -3), value.transpose(-2, -3), is_causal=self.is_causal).transpose(-2, -3)\n",
    "        # attn_output: [..., seq_len, num_heads, head_dim]\n",
    "        attn_output = rearrange(attn_output, \"... h hd -> ... (h hd)\")\n",
    "        return self.o_proj(attn_output)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: HRMConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.attn = Attention(\n",
    "            hidden_size=config.hidden_size,\n",
    "            head_dim=config.head_dim,\n",
    "            num_heads=config.hidden_size // config.head_dim,\n",
    "            is_causal=config.is_causal\n",
    "        )\n",
    "        self.mlp = SwiGLU(\n",
    "            hidden_size=config.hidden_size,\n",
    "            intermediate_size=config.intermediate_size\n",
    "        )\n",
    "        self.norm = lambda x: F.rms_norm(x, (x.shape[-1], ), eps=config.norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs) -> torch.Tensor:  # Post Norm\n",
    "        x = self.norm(x + self.attn(x, **kwargs))\n",
    "        return self.norm(x + self.mlp(x))\n",
    "\n",
    "class HRMRecurrentBlock(nn.Module):\n",
    "    def __init__(self, config: HRMConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([TransformerBlock(config) for _layer_idx in range(config.num_layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, n: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        h = x + n\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, **kwargs)\n",
    "        return h\n",
    "\n",
    "# HRMCarry is a tuple containing two latent states(z_H, z_L)\n",
    "HRMCarry = Tuple[torch.Tensor, torch.Tensor]\n",
    "\n",
    "class HRM(nn.Module):\n",
    "    def __init__(self, config: HRMConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.H_cycles = config.H_cycles\n",
    "        self.L_cycles = config.L_cycles\n",
    "\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.seq_len = config.seq_len\n",
    "        self.dtype = getattr(torch, config.forward_dtype)\n",
    "\n",
    "        # Backbone Layers\n",
    "        self.H_level = HRMRecurrentBlock(config)\n",
    "        self.L_level = HRMRecurrentBlock(config)\n",
    "        \n",
    "        # RoPE\n",
    "        self.rope = RotaryEmbedding(config.head_dim, config.seq_len, config.rope_base)\n",
    "        # I/O Layers\n",
    "        self.embed = CastedScaledEmbedding(config.vocab_size, config.hidden_size, cast_to=self.dtype)\n",
    "        self.lm_head = CastedLinear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "    def initial_carry(self, batch_size: int):\n",
    "        z_H = trunc_normal_init_(torch.empty(1, 1, self.hidden_size, dtype=self.dtype), std=1.0).expand(batch_size, self.seq_len, -1)\n",
    "        z_L = trunc_normal_init_(torch.empty(1, 1, self.hidden_size, dtype=self.dtype), std=1.0).expand(batch_size, self.seq_len, -1)\n",
    "        return (z_H, z_L)\n",
    "\n",
    "    def forward(self, carry: HRMCarry, input_ids: torch.Tensor) -> Tuple[HRMCarry, torch.Tensor]:\n",
    "        x = self.embed(input_ids)\n",
    "        seq_info = dict(cos_sin=self.rope())\n",
    "\n",
    "        # Forward iterations\n",
    "        with torch.no_grad():\n",
    "            z_H, z_L = carry  # Unpack tuple\n",
    "            for _i in range(self.H_cycles * self.L_cycles - 1):\n",
    "                z_L = self.L_level(z_L, z_H + x, **seq_info)\n",
    "                if (_i + 1) % self.L_cycles == 0:\n",
    "                    z_H = self.H_level(z_H, z_L, **seq_info)\n",
    "\n",
    "        assert not z_H.requires_grad and not z_L.requires_grad\n",
    "\n",
    "        # 1-step grad\n",
    "        z_L = self.L_level(z_L, z_H + x, **seq_info)\n",
    "        z_H = self.H_level(z_H, z_L, **seq_info)\n",
    "        return (z_H.detach(), z_L.detach()), self.lm_head(z_H)  # Return tuple and ensure no gradient moves across carry"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T08:13:35.754015Z",
     "start_time": "2025-10-15T08:13:35.727558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# init model and load parameter\n",
    "\n",
    "hrm_config = HRMConfig()\n",
    "\n",
    "set_up(hrm_config.seed)\n",
    "\n",
    "device = torch.accelerator.current_accelerator(check_available=True)\n",
    "if device is None:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "with torch.device(device):\n",
    "    model = HRM(hrm_config)\n",
    "\n",
    "# load parameter\n",
    "model_path='HRM_Mini.pth'\n",
    "state_dict = torch.load(model_path, map_location=device)\n",
    "# as we compile the model. so here we remove the prefix\n",
    "model.load_state_dict({k.replace(\"_orig_mod.\", \"\"): v for k, v in state_dict.items()}, strict=True)"
   ],
   "id": "4e183c2411c8a8b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T08:15:55.260522Z",
     "start_time": "2025-10-15T08:15:54.984840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "sudoku_str = \"7..6....8...9..5....6.4..3.23...8.....4...6.....7...91.2..3.4....1..9...8....1..5\"\n",
    "\n",
    "sudoku_array = np.frombuffer(sudoku_str.replace('.', '0').encode(), dtype=np.uint8).reshape(9, 9) - ord('0')\n",
    "sudoku_array = sudoku_array.flatten().astype(np.int32)\n",
    "sudoku_array = np.pad(sudoku_array, (1, 0))\n",
    "\n",
    "def is_valid_sudoku(board):\n",
    "    check = lambda units: all(set(unit) == set([1,2,3,4,5,6,7,8,9]) for unit in units)\n",
    "    rows = [board[i*9:(i+1)*9] for i in range(9)]\n",
    "    cols = [board[j::9] for j in range(9)]\n",
    "    boxes = [[board[(r+i)*9+(c+j)] for i in range(3) for j in range(3)] for r in [0,3,6] for c in [0,3,6]]\n",
    "    return check(rows) and check(cols) and check(boxes)\n",
    "\n",
    "\n",
    "with torch.device(device):\n",
    "    carry = model.initial_carry(batch_size=1)\n",
    "    # prepare board tensor\n",
    "\n",
    "sudoku_tensor = torch.from_numpy(sudoku_array).unsqueeze(0).to(device)\n",
    "# initialize carry with batch_size = 1\n",
    "# cycle means how many time we run hrm model\n",
    "cycles = 16\n",
    "\n",
    "for _ in range(cycles):\n",
    "    carry, y_hat = model(carry, sudoku_tensor)\n",
    "answer = torch.argmax(y_hat, dim=-1).squeeze(0).tolist()[1:]\n",
    "\n",
    "print(f'puzzle: {sudoku_str}')\n",
    "print(f'answer: {''.join(map(str, answer))}')\n",
    "print(\"is_valid_sudoku: \",is_valid_sudoku(answer))"
   ],
   "id": "83a17ead0055feab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "puzzle: 7..6....8...9..5....6.4..3.23...8.....4...6.....7...91.2..3.4....1..9...8....1..5\n",
      "answer: 745613928312987546986542137239168754174395682658724391527836419461259873893471265\n",
      "is_valid_sudoku:  True\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T08:13:36.401471Z",
     "start_time": "2025-10-15T08:13:36.400010Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a8e78113d3a077fa",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hrm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
